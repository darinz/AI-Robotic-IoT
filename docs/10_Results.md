# RESULTS:

## IOT: REMOTE HOME AUTOMATION AND CONTROL

Overall, the system functioned successfully, meeting its core objectives of providing remote and automated control of bedroom lighting through sound-based commands and a dedicated remote/mobile application. Our personal interaction with the system showed that it was practical and usable, as we could reliably use both claps and the remote/mobile app to control our bedroom lights without the need for cloud services.

However, several performance issues emerged during testing. Specifically, the clap-detection component, utilizing the KY-037 sound sensor, proved to be somewhat unreliable. Although it effectively recognized single and double claps in ideal conditions, it was sensitive to ambient noise such as doors opening, footsteps, or loud sounds from adjacent rooms. In addition, it occasionally failed to register legitimate claps if they were not sufficiently loud or distinct, leading to an inconsistent user experience.
 
The remote/mobile application consistently provided accurate status updates and reliable command execution when within optimal Wi-Fi coverage. However, it became less reliable in areas of weaker Wi-Fi signal within a home, occasionally failing to communicate with the Raspberry Pi-based MQTT broker, resulting in unresponsive controls.

In the cloud-based approach, the implementation of AWS IoT Core integration successfully extended the functionality of our remote home automation system beyond the local network, enabling true remote control from anywhere with internet access. Testing showed that the mobile application reliably connected to AWS IoT Core, consistently publishing commands that were immediately relayed to the Raspberry Pi and executed locally.

However, the two main issues that we faced with this approach are the complexity of handling certificates and latency. Handling secure MQTT connections via manual packet construction proved initially challenging. Managing certificates and ensuring secure authentication required a lot of time to implement and often lost connection. There was also increased latency when pushing the buttons compared to the local approach depending on how the cellular signal is at a particular location or time of the day.

### From these results, several important lessons emerged:

#### Component Quality Matters: 

Lower-cost sound sensors like the KY-037, though affordable and easy to use, come with inherent limitations in sensitivity and accuracy. We had to adjust the potentiometer on the sensor multiple times and moving it even slightly in one direction can affect the sensitivity greatly. For future projects, investing in higher-quality audio sensors or incorporating advanced audio signal processing would significantly improve reliability, or a microphone for more advanced sound filtering.

#### Network Stability is Critical: 

The intermittent Wi-Fi connectivity issues highlighted the need for more robust and consistent network infrastructure, especially for real-time IoT applications. Some possibilities are deploying Wi-Fi extenders, or employing protocols designed to handle unstable connections with error handling could help enhance reliability. On the cloud end, cellular signal can sometimes be unreliable depending on the user’s location.

#### User Experience Considerations:

Even a simple system like this one need careful consideration of environmental variables and real-world scenarios. Sensitivity thresholds, debounce logic, and timing parameters must be fine-tuned based on thorough, realistic testing to avoid accidental triggers or missed interactions. As the system grows and more sensors are placed around the home, a method needs to be developed to isolate the sound detection in different areas of the home.
 
The primary takeaway from my results is that while integrating IoT technology can yield practical and convenient solutions, attention to hardware quality, network infrastructure, and thorough real-world testing is essential. Understanding these limitations and addressing them proactively leads to more reliable, satisfying, and practical IoT systems in real-world deployments.

## RL: USING REINFORCEMENT LEARNING TO BUILD THE SELF-DRIVING SYSTEM

The final implementation successfully demonstrated a reinforcement learning-based self-driving robotic car capable of navigating simple environments using Q-learning. After many rounds of training in a controlled space, the vehicle learned to avoid most obstacles and exhibited clear improvements in its decision-making ability. It was able to move forward confidently, turn when needed, and generally steer clear of direct collisions. Persistent storage of the Q-table allowed the car to accumulate experience across days, enabling cumulative improvement and fine-tuning of navigation behavior.

Despite these successes, several challenges significantly limited overall performance. Most notably, we encountered serious hardware constraints that restricted our ability to implement more advanced algorithms. Although we originally intended to use Deep Q-Networks (DQN) for richer state representation and better generalization, the Raspberry Pi simply lacked the computing power required for real-time deep learning. Without a GPU or sufficient memory, the DQN implementation could not run on the Raspberry Pi, forcing us to rely exclusively on tabular Q-learning using NumPy. This limited us to very simple state spaces derived from the ultrasonic sensor and made it impractical to integrate camera-based input.

The Raspberry Pi camera module, which we hoped to use for visual perception, proved unreliable and difficult to integrate into our system. It frequently failed to initialize or stream consistently, and when it did work, processing even low-resolution frames in real time placed too much strain on the processor. As a result, the camera was ultimately not used for navigation and obstacle avoidance, and all perception was based on ultrasonic distance measurements alone.

To accommodate the limited responsiveness of the system and to ensure safe experimentation, we deliberately slowed down the vehicle’s movement during training. This helped prevent damage during early learning stages, when the agent was still randomly exploring its environment, and gave the RL model more time to receive and react to sensor input. While this made training safer and more stable, it also made learning slower and less responsive, as each action took longer to execute.

There were also limitations in behavior generalization. Although the car could generally avoid obstacles, it occasionally failed to correct its course and continued into walls or corners, even after extended training. These failure cases highlighted the limits of a discrete state-action space and the challenges of reward shaping. Tuning the reward function and hyperparameters like learning rate and exploration decay was particularly challenging, requiring many iterations to balance learning efficiency and safety.

Another major issue stemmed from the hardware setup, particularly the sensor configuration. Our ultrasonic sensor and camera were mounted to face forward only. This setup allowed for reasonable detection of obstacles in the path ahead, but it left the rear and sides of the car blind. Consequently, when the agent chose to back up in response to an obstacle, it had no way to perceive what was behind it. In several instances, the car became stuck in a loop where it continuously backed into a wall or object without being able to recognize or correct the behavior. This highlighted the importance of having 360-degree environmental perception for robust navigation.

Despite these hurdles, the project was an informative and hands-on exploration of reinforcement learning in the real world. We learned how online learning, reward shaping, and exploration strategies can influence agent behavior. More importantly, we gained a realistic appreciation for the challenges of implementing RL on physical hardware—where sensor noise, latency, and resource constraints introduce real-world complexity absent in simulations.

The takeaway is that reinforcement learning can work on embedded systems, but success depends heavily on thoughtful simplification, careful tuning, and iterative testing. For future improvements, offloading perception and learning to a more powerful compute unit or integrating external controllers (e.g., via Wi-Fi or edge TPU) could enable the use of deep learning and more complex perception systems, such as camera-based object detection or semantic mapping.

## NLP: NATURAL LANGUAGE PROCESSING AND CONVERSATIONAL INTERFACE RESULTS

The implementation yielded a fully functional conversational interface with both audio and physical feedback, capable of engaging users in natural language interactions and triggering robotic behaviors based on interpreted commands. In terms of performance, commercial LLMs like OpenAI’s GPT-4 and GPT-4o clearly outperformed local open-source models. These cloud-based models produced more fluent, contextually aware, and personality-consistent responses—crucial for simulating a character like KITT. They handled nuanced prompts, humor, and ambiguity with ease, which was essential for user engagement.

Conversely, open-source models such as LLaMA, even when quantized and served from a local LAN server, often fell short in open-domain dialogue tasks. Responses tended to be more generic, less context-aware, and sometimes failed to follow subtle cues or user intentions. This became especially apparent when using open-ended prompts or attempting personality simulation without extensive prompt tuning. These models were more useful for structured or command-like interactions but lacked the richness and consistency of the commercial models.

Another challenge encountered was hardware and resource limitations. Running large LLMs locally demanded significant compute power, and although we had access to GPU-enabled machines, they were not optimized for large-scale inference. As a result, inference latency could spike beyond usable limits, and memory constraints sometimes caused crashes or degraded performance. These issues underscored the trade-off between control/cost and performance/reliability when deciding between local and commercial models.

We also learned that prompt engineering is crucial. Even with powerful models, poorly designed prompts led to underwhelming results. Iterative refinement of prompt templates was required to coax high-quality, actionable responses—especially from the local models. This trial-and-error process taught us the value of engineering prompts with clear intent, emotional framing, and fallback strategies.

Despite the setbacks, the system succeeded in creating an interactive, speech-driven robotic experience with a layered response architecture that merged voice, logic, and action. The key takeaway is that commercial LLMs currently provide unmatched fluency and reliability, but open-source models hold promise for offline or constrained environments if paired with clever optimization and prompt strategies. We also learned that speech interfaces are inherently noisy and ambiguous, so designing robust fallback mechanisms and clarification flows is vital for a seamless user experience.

Overall, the project revealed both the power and the limitations of modern LLMs in IoT embedded systems. It provided firsthand insights into balancing computation, latency, and quality, and showed that effective natural language interfaces require more than just plugging in a model—they demand orchestration, error handling, and thoughtful integration into the physical world.

## CV: ADDING COMPUTER VISION CAPABILITIES RESULTS

The implementation of computer vision in KITT yielded a mix of valuable insights and technical frustrations. On the positive side, we achieved the foundational goal of enabling KITT to "see" its environment and describe it using natural language. Using the integrated pipeline of object detection, OCR, and NLP, KITT successfully described an object it saw (a tissue box) in natural language and correctly read and vocalized printed text from a tissue box using OCR. These successful demonstrations showcased the potential of combining computer vision with language models to create a perception-aware agent that can reason and communicate about its visual environment in a human-like way.

However, the road to that result was filled with significant hardware and software hurdles. The most persistent problem was the instability of the camera module. The ribbon cable connection between the Raspberry Pi and the camera would intermittently fail, causing the video stream to crash and making real-time vision unreliable. Moreover, the Raspberry Pi 4’s limited processing power severely constrained our ability to run advanced models or even perform multiple tasks simultaneously. Libraries like OpenCV, Tesseract, and PyTorch, while powerful, were taxing on the Pi, often leading to performance bottlenecks and resource contention.

The takeaway is that while edge-based computer vision with language integration is possible, it is highly sensitive to both hardware stability and system resource management. Even a small demonstration of an AI agent describing its visual input or reading signs is powerful—it paves the way for autonomous agents that can understand and navigate the world more naturally. However, deploying such systems on lightweight hardware like a Raspberry Pi requires careful trade-offs, modular architecture, and in many cases, offloading compute to external machines. The key lesson learned is that hardware reliability must be addressed early, and system modularity is critical for isolating failures and scaling capabilities. Future iterations will need more robust camera modules and potentially more powerful edge AI platforms like the NVIDIA series to realize the full vision of self-driving, vision-enabled conversational robots.